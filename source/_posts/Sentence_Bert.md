---
title: Sentence Bert
date: 2023-09-11
categories:
  - Paper
  - NLP
tags: 
  - Sentence Bert
  - Bi Encoder
  - Cross Encoder
---

# ë“¤ì–´ê°€ë©°

ì´ ê¸€ì€ Sentence-BERT: Sentence Embeddings using Siamese BERT-Networksë¥¼
ì†Œê°œí•˜ê³  ë…¼ë¬¸ì˜ í•µì‹¬ êµ¬ì¡°ì¸ Sbertë¥¼ ì½”ë“œë¡œ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´
ì„¤ëª…í•©ë‹ˆë‹¤.



# Sentence Bertê°€ í•„ìš”í•œ ì´ìœ 

Sentence BertëŠ” Bertì„ ë¬¸ì¥ ì„ë² ë”©(Sentence Embedding)ì„ ìƒì„±í•˜ëŠ” ëª¨ë¸ë¡œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ Fine-tuningí•˜ëŠ” ë°©ë²•(ë˜ëŠ” ëª¨ë¸ëª…) ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ë•Œ Sentence embeddingë¼ í•¨ì€ ë¬¸ì¥ ì •ë³´ë¥¼ ë²¡í„° ê³µê°„ì˜ ìœ„ì¹˜ë¡œ í‘œí˜„í•œ ê°’ì„ ë§í•˜ë©°, ë¬¸ì¥ì„ ë²¡í„° ê³µê°„ì— ë°°ì¹˜í•¨ìœ¼ë¡œì„œ ë¬¸ì¥ ê°„ ë¹„êµ, í´ëŸ¬ìŠ¤í„°ë§, ì‹œê°í™” ë“± ë‹¤ì–‘í•œ ë¶„ì„ ê¸°ë²•ì„ ì´ìš©í•  ìˆ˜ ìˆëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.

ì‚¬ì‹¤ Sbert ì´ì „ì—ë„ Bert ëª¨ë¸ì„ í™œìš©í•´ Sentence Embeddingì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì´ ì¡´ì¬í–ˆì§€ë§Œ, ì´ëŸ¬í•œ ë°©ë²•ì€ ê³¼ê±° ëª¨ë¸(Glove,Infer-Sent)ì˜ ì„±ëŠ¥ì— ë¯¸ì¹˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì´ìœ  ë•Œë¬¸ì— Transformer ê¸°ë°˜ ëª¨ë¸ì„ í™œìš©í•´ ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ë¥¼ ë¹„êµí•˜ëŠ” Taskì—ì„œëŠ” sentence embedding ë°©ë²•ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì£¼ë¡œ ë‘ ê°œì˜ ë¬¸ì¥ì„ ëª¨ë¸ì— ë„£ì–´ Cross-Attentionì„ í™œìš©í•´ ë¹„êµí•˜ëŠ” ë°©ì‹ì„ í™œìš©í–ˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ì¼ëŒ€ì¼ë¡œ ë°©ì‹ì´ë¼ í•˜ë©´ ë‘ ê°œì˜ ë¬¸ì¥ì„ í•˜ë‚˜ë¡œ ë¬¶ì€ Input Dataë¥¼ Bert ëª¨ë¸ì— ë„£ì€ ë’¤ ëª¨ë¸ ë‚´ë¶€ì—ì„œ ë‘ ë¬¸ì¥ ê°„ ê´€ê³„ë¥¼ íŒŒì•…í•˜ê³  ëª¨ë¸ì˜ Output ì¤‘  [CLS] í† í°ì„ í™œìš©í•´ ë‘ ë¬¸ì¥ì˜ ìœ ì‚¬ë„ë¥¼ íŒŒì•…í•˜ëŠ” ë°©ë²•ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

Sentence Bert ë…¼ë¬¸ì—ì„œëŠ” ë¬¸ì¥ê³¼ ë¬¸ì¥ì„ ë¹„êµí•˜ëŠ” Taskì¸ Named Entity Recognition(NER), Semantic Textual Similarity(STS)ë¥¼ ìˆ˜í–‰í•˜ëŠ”ë° Senetnece Embeddingì„ í™œìš©í•˜ê³  ìˆì§€ë§Œ, Senetence Embeddingì€ ì´ëŸ¬í•œ Task ë¿ë§Œì•„ë‹ˆë¼ ë¬¸ì¥ê³¼ ë‹¨ì–´ ê°„ ì—°ê´€ì„± ë¹„êµë¥¼ í†µí•œ í‚¤ì›Œë“œ ì¶”ì¶œ, íŠ¹ì • ë¬¸ì„œì˜ ì¹´í…Œê³ ë¦¬ ì„ ì • ë“± ë‹¤ì–‘í•œ Taskì—ì„œ ì‘ìš©ì´ ê°€ëŠ¥í•˜ë¯€ë¡œ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œí•œ ë…¼ë¬¸ì´ë‚˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ë‹¤ìŒì˜ ë§í¬ë“¤ì€ Setnece Bertë¥¼ í™œìš©í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë…¼ë¬¸ë“¤ì…ë‹ˆë‹¤.

-   [Sbert ê³µì‹ í˜ì´ì§€ ì‘ìš© ì˜ˆì‹œ](https://www.sbert.net/examples/applications/)
-   [Bertopic : í† í”½ ì¶”ì¶œ ë¼ì´ë¸ŒëŸ¬ë¦¬](https://github.com/MaartenGr/BERTopic)
-   [keyBert : ë¬¸ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ë¼ì´ë¸ŒëŸ¬ë¦¬](https://github.com/MaartenGr/BERTopic)

# Cross-Encoderì™€ Bi-Encoder

í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” Bert ëª¨ë¸ ë‚´ë¶€ì˜ Cross-Ateentionì„ í™œìš©í•´ ë¬¸ì¥ ê°„ ê´€ê³„ë¥¼ ë¹„êµí–ˆë˜ ê¸°ì¡´ ë°©ì‹ì„ Cross-Encoderë¼ëŠ” ìš©ì–´ë¡œ ì‚¬ìš©í•˜ê³  ìˆìœ¼ë©°, ë…¼ë¬¸ì—ì„œ ìƒˆë¡­ê²Œ ì†Œê°œí•˜ëŠ” êµ¬ì¡°ë¥¼ Bi-Encoderë¼ëŠ” ìš©ì–´ë¡œ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. 

Cross-Encoderì™€ Bi-Encoderì˜ êµ¬ì¡° ì°¨ì´ëŠ” ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ìŠµë‹ˆë‹¤

<p align = "center"><img src="https://yangoos57.github.io/static/812fe66e9ad7a89e832b77f4cf7a8c27/3c492/img0.png" width="500" height="500"/></p>


ìœ„ ê·¸ë¦¼ì— ëŒ€í•´ ì„¤ëª…í•˜ë©´, Bi-EncoderëŠ” ë‘ ë¬¸ì¥ì„ ë¹„êµí•˜ê¸° ìœ„í•´ ê°œë³„ ë¬¸ì¥ì˜ Embedding ìƒì„±í•˜ëŠ” ë‹¨ê³„ -> ëª¨ë¸ Outputì„ Poolingí•˜ì—¬ Sentence Embedding ìƒì„±í•˜ëŠ” ë‹¨ê³„ -> CosineSimilarityë¥¼ í†µí•´ ë¬¸ì¥ê³¼ ë¬¸ì¥ ê°„ ê´€ê³„ ë¹„êµë¥¼ ë¹„êµí•˜ëŠ” ë‹¨ê³„ ì´ë ‡ê²Œ 3ë²ˆì˜ ë‹¨ê³„ë¥¼ ê±°ì¹©ë‹ˆë‹¤. ê¸°ì¡´ ë°©ì‹ì¸ Cross-EncoderëŠ” ë‘ ê°œì˜ ë¬¸ì¥ì„ Language Modelì— ë„£ì–´ ë‚´ë¶€ì—ì„œ ë¬¸ì¥ ê°„ ë¬¸ì¥ì˜ ê´€ê³„ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.

ì ˆì°¨ì  ì¸¡ë©´ì—ì„œ ë³´ë©´ Cross-Encoderê°€ ë” ê°„ë‹¨í•œ ë°©ë²•ì¸ ê²ƒ ê°™ì•„ ë³´ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ 100ê°œ ë¬¸ì¥ì„ ë¹„êµí•œë‹¤ê³  ê°€ì •í•  ë•Œ Cross-EncoderëŠ” 100ê°œì˜ ë¬¸ì¥ì„ 1:1ë¡œ ë¹„êµí•´ì•¼ í•˜ë¯€ë¡œ 100C2íšŒë¥¼ ìˆ˜í–‰í•´ì•¼ í•˜ëŠ” ë°˜ë©´ Bi-EncoderëŠ” ì¼ë‹¨ ë¬¸ì¥ì„ embeddingí•˜ë©´ ë¹„êµí•˜ëŠ” ê³¼ì • ìì²´ëŠ” ë‹¨ìˆœí•˜ë¯€ë¡œ ë¬¸ì¥ì„ embeddingí™” í•˜ê¸° ìœ„í•´ 100íšŒë§Œ ìˆ˜í–‰í•˜ë©´ ë©ë‹ˆë‹¤. êµ¬ì¡° ìì²´ëŠ” Cross-Encoderê°€ ë‹¨ìˆœí•´ë³´ì´ì§€ë§Œ ì‹¤ì œë¡œëŠ” Bi-Encoder ë°©ì‹ì´ íš¨ìœ¨ì„± ë©´ì—ì„œ í›¨ì”¬ ë” íš¨ê³¼ì ì„ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Cross-Encoderì™€ Bi-Encoderì— ëŒ€í•´ ê°œë³„ì ìœ¼ë¡œ ì•Œì•„ë³´ê¸° ì „ Cross-Encoderì™€ Bi-Encoderì˜ íŠ¹ì§•ì— ëŒ€í•´ ê°„ë‹¨íˆ ì•Œì•„ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ë¨¼ì € Cross-EncoderëŠ” ë¬¸ì¥ ê°„ ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ” ì„±ëŠ¥ì´ ìš°ìˆ˜í•œ ì¥ì ì´ ìˆì§€ë§Œ ì•ì„œ ì„¤ëª…í–ˆë“¯ ë¹„êµí•´ì•¼í•˜ëŠ” ë¬¸ì¥ìˆ˜ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ì—°ì‚°ì´ ê¸‰ì¦í•œë‹¤ëŠ” ì¹˜ëª…ì ì¸ ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤. ë°˜ë©´ Bi-EncoderëŠ” Embedding ê³¼ì •ì—ì„œ ì •ë³´ì†ì‹¤ì´ ë°œìƒí•˜ë¯€ë¡œ ì„±ëŠ¥ì— ìˆì–´ì„œ Cross-Encoderì— ë¯¸ì¹˜ì§€ ëª»í•˜ì§€ë§Œ, ì‹¤ì‹œê°„ ë¬¸ì œ í•´ê²°ì— í™œìš©ë  ìˆ˜ ìˆì„ë§Œí•œ ë¹ ë¥¸ ì—°ì‚° ì†ë„ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.

ì´ëŸ¬í•œ íŠ¹ì§•ì—ì„œ ë³´ë“¯ ì´ ë‘˜ì€ ìƒí˜¸ ë³´ì™„ì ì¸ ê´€ê³„ì— ìˆìŠµë‹ˆë‹¤. Bi-EncoderëŠ” Cross-Encoderì˜ ëŠë¦° ì—°ì‚°ì†ë„ë¥¼ ë³´ì™„í•  ìˆ˜ ìˆê³ , Cross-EncoderëŠ” Bi-Encoderì˜ ë¶€ì¡±í•œ ë¬¸ì¥ ë¹„êµ ì„±ëŠ¥ì„ ë³´ì™„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¤ì œë¡œë„ ì´ëŸ¬í•œ
ê°œë³„ íŠ¹ì§•ì„ í™œìš©í•´ ê²€ìƒ‰ ê¸°ëŠ¥ì„ êµ¬í˜„í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ Bi-Encoderì™€ Cross-Encoderì˜ ê°œë³„ ì¥ì ì„ ì‚´ë ¤ íš¨ê³¼ì ì¸ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ êµ¬ì¡°ëŠ” Bi-Encoderì˜ ë¹ ë¥¸ ì—°ì‚°ì†ë„ë¥¼ í™œìš©í•´ queryì™€ ìœ ì‚¬í•œ ë¬¸ì¥ì„ ì¶”ë ¤ë‚¸ ë‹¤ìŒ, Cross-Encoderë¥¼ í™œìš©í•´ ì¶”ë ¤ë‚¸ ë¬¸ì¥ê³¼ Query ê°„ ì—°ê´€ì„±ì„ ë‹¤ì‹œ ê³„ì‚°í•´ ìˆœìœ„ë¥¼ ë©”ê¸°ëŠ” ë°©ì‹ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.

> ì œê°€ ìˆ˜í–‰í–ˆë˜ ë¯¸ë‹ˆí”„ë¡œì íŠ¸ì¸ [Sentence Bertë¥¼ í™œìš©í•´ ì—°ê´€ì„± ë†’ì€ ë„ì„œ ì¶”ì²œí•˜ê¸°](https://github.com/yangoos57/Sentence_bert_from_scratch)ë¥¼ ì½ì–´ë³´ë©´ ì´ëŸ¬í•œ êµ¬ì¡°ë¥¼ ì–´ë–»ê²Œ ì½”ë“œë¡œ êµ¬í˜„í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
 <p align = "center"><img src="https://yangoos57.github.io/static/31659fa96212160ec5c5ec892af7e5d1/3c492/img1.png" width="600" height="300"/></p>



## Cross-Encoder

ë¨¼ì € ê¸°ì¡´ ë°©ì‹ì¸ Cross-Encoderì— ëŒ€í•´ì„œ ì„¤ëª…í•œ ë’¤, ë…¼ë¬¸ì—ì„œ ì†Œê°œí•˜ëŠ” Bi-Encoderì— ëŒ€í•´ì„œ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.

### â– Cross-Encoder êµ¬ì¡° ì´í•´í•˜ê¸°

Cross-Encoder êµ¬ì¡°ëŠ” Language Modelì— classification layerë¥¼ ìŒ“ì€ êµ¬ì¡°ì…ë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì—ì„œ íŒŒë€ìƒ‰ ë„¤ëª¨ ë°•ìŠ¤ë¥¼ Language Modelì´ë¼ í•˜ë©° ê·¸ ìœ„ì˜ ë…¸ë€ìƒ‰ í…Œë‘ë¦¬ë¥¼ Classification Layerë¼ í•©ë‹ˆë‹¤. Language Modelì€ Bert ë¿ë§Œì•„ë‹ˆë¼ Electra, Roberta ë“± Encoder ê¸°ë°˜ ëª¨ë¸ì´ë©´ ëª¨ë‘ í™œìš©í•  ìˆ˜
ìˆìŠµë‹ˆë‹¤.


<p align = "center"><img src="https://yangoos57.github.io/static/768bc61ae0bef22c4c25914cb3393e76/3c492/img7.png" width="600" height="500"/></p>


Cross-Encoder ë‚´ë¶€ì˜ ë°ì´í„° íë¦„ì„ ë³´ë©´ Language Modelì˜ Outputì„ ì‚°ì¶œí•œ ë’¤ CLS Poolingì„ ê±°ì³ ë‹¤ì‹œ Classification Layerì˜ Input Dataë¡œ í™œìš©ë˜ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë•Œ CLS poolingì´ë¼ í•˜ë©´ ë¬¸ì¥ì˜ ì—¬ëŸ¬ token embedding ì¤‘ \[CLS\] token embeddingì„ ë¬¸ì¥ embeddingìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. CLS Poolingì„ ë‹¤ë¥´ê²Œ í‘œí˜„í•˜ìë©´ ë¬¸ì¥ê³¼ ë¬¸ì¥ì˜ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ê³  ìˆëŠ” ì •ë³´ë“¤ì€ \[CLS\] tokenì— ëª¨ë‘ ë…¹ì•„ë“¤ì–´ìˆìœ¼ë‹ˆ \[CLS\] tokenì™¸ ë‚˜ë¨¸ì§€ëŠ” ë¬¸ì¥ embeddingìœ¼ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤ë¼ëŠ” ì˜ë¯¸ë¡œ ì´í•´í•˜ì‹œë©´ ë˜ê² ìŠµë‹ˆë‹¤.

Cross-Encoderì˜ êµ¬ì¡°ëŠ” Language Modelê³¼ Classification Headë¡œ êµ¬ì„±ëœ ë§¤ìš° ê°„ë‹¨í•œ êµ¬ì¡°ì´ë©° ì•„ë˜ì˜ ì½”ë“œëŠ” ì´ëŸ¬í•œ êµ¬ì¡°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì•„ë˜ ì½”ë“œì—ì„œ ì£¼ëª©í•´ì•¼í•  ì ì€ argumentsë¡œ í™œìš©ë˜ëŠ” num_labelsì˜ ì¡´ì¬ì…ë‹ˆë‹¤.

Cross-Encoder Classì—ì„œ num_labelsê°€ í™œìš©ë˜ëŠ” ëª©ì ì€ ëª¨ë¸ì˜ Loss Functionì„ ì ìš©í•˜ëŠ”ë° ìˆìŠµë‹ˆë‹¤. ì½”ë“œ ë§ˆì§€ë§‰ ë¶€ë¶„ì—ì„œ num_labelsê°€ í™œìš©ë˜ëŠ” ì½”ë“œë¥¼ ë³¼ ìˆ˜ ìˆëŠ”ë°, num_labelsì´ 1ì¸ ê²½ìš° MSEë¥¼ Loss functionì„ í™œìš©í•˜ê³  ê·¸ì™¸ì¸ ê²½ìš° Cross Entropyë¥¼ Loss functionìœ¼ë¡œ í™œìš©í•˜ê³  ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. num_labels ê°’ì— ë”°ë¼ Loss functionì´ ë‹¬ë¼ì§€ëŠ” ì´ìœ ëŠ” input Dataë¡œ ì‚¬ìš©ë˜ëŠ” íƒ€ì…ì´ Numerical Dataì¸ì§€ Categorical Dataì¸ì§€ ì—¬ë¶€ì— ë”°ë¼ ì‚¬ìš©í•´ì•¼í•˜ëŠ” Loss functionì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

 
``` python
from torch.nn import CrossEntropyLoss, MSELoss

class CrossEncoder(nn.Module):
    def __init__(self, model, num_labels) -> None:
        super().__init__()
        self.model = model
        self.model.config.num_labels = num_labels
        self.classifier = classificationHead(self.model.config)
    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        labels=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ):
        model = self.model(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        # Last-hidden-states ì¶”ì¶œ
        sequence_output = model[0]
        # classificationHeadì— Last-hidden-state ëŒ€ì…
        logits = self.classifier(sequence_output)
        loss = None
        if labels is not None:
            if self.model.config.num_labels == 1:
                # Regression Modelì€ MSE Loss í™œìš©
                loss_fct = MSELoss()
            else:
                # classification Modelì€ Cross entropy í™œìš©
                loss_fct = CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, 3), labels.view(-1))
            return {"loss": loss, "logit": logits}
        else:
            return {"logit": logits}

```



**CLS í† í°ì´ë€?**

-   BERTëŠ” í•™ìŠµì„ ìœ„í•´ ê¸°ì¡´ transformerì˜ input êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ë©´ì„œë„ ì¶”ê°€ë¡œ ë³€í˜•í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤. Tokenizationì€ WorldPiece ë°©ë²•ì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.
<p align = "center"><img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FpZneZ%2FbtqGg6mCUaU%2FEcXXk5nCUAdTRMK2vXORO0%2Fimg.png" width="700" height="300"/></p>

-   ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ ì„¸ ê°€ì§€ ì„ë² ë”©(Token, Segment, Position)ì„ ì‚¬ìš©í•´ì„œ ë¬¸ì¥ì„ í‘œí˜„í•©ë‹ˆë‹¤.

-   ë¨¼ì € Token Embeddingì—ì„œëŠ” ë‘ ê°€ì§€ íŠ¹ìˆ˜ í† í°(CLS, SEP)ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì„ êµ¬ë³„í•˜ê²Œ ë˜ëŠ”ë°ìš”. Special Classification token(CLS)ì€ ëª¨ë“  ë¬¸ì¥ì˜ ê°€ì¥ ì²« ë²ˆì§¸(ë¬¸ì¥ì˜ ì‹œì‘) í† í°ìœ¼ë¡œ ì‚½ì…ë©ë‹ˆë‹¤. ì´ í† í°ì€ Classification taskì—ì„œëŠ” ì‚¬ìš©ë˜ì§€ë§Œ, ê·¸ë ‡ì§€ ì•Šì„ ê²½ìš°ì—” ë¬´ì‹œë©ë‹ˆë‹¤.
-   ë˜, Special Separator token(SEP)ì„ ì‚¬ìš©í•˜ì—¬ ì²« ë²ˆì§¸ ë¬¸ì¥ê³¼ ë‘ ë²ˆì§¸ ë¬¸ì¥ì„ êµ¬ë³„í•©ë‹ˆë‹¤. ì—¬ê¸°ì— segment Embeddingì„ ë”í•´ì„œ ì•ë’¤ ë¬¸ì¥ì„ ë”ìš± ì‰½ê²Œ êµ¬ë³„í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤. ì´ í† í°ì€ ê° ë¬¸ì¥ì˜ ëì— ì‚½ì…ë©ë‹ˆë‹¤.
-   Position Embeddingì€ transformer êµ¬ì¡°ì—ì„œë„ ì‚¬ìš©ëœ ë°©ë²•ìœ¼ë¡œ ê·¸ë¦¼ê³¼ ê°™ì´ ê° í† í°ì˜ ìœ„ì¹˜ë¥¼ ì•Œë ¤ì£¼ëŠ” ì„ë² ë”©ì…ë‹ˆë‹¤. ìµœì¢…ì ìœ¼ë¡œ ì„¸ ê°€ì§€ ì„ë² ë”©ì„ ë”í•œ ì„ë² ë”©ì„ inputìœ¼ë¡œ ì‚¬ìš©í•˜ê²Œ ë©ë‹ˆë‹¤.



### â– Classification layer êµ¬ì¡° ì´í•´í•˜ê¸°

Cross-Encoderì˜ ì „ì²´ êµ¬ì¡°ì™€ ì½”ë“œë¥¼ ì†Œê°œí–ˆìœ¼ë‹ˆ ì´ì œ Classification Layerì˜ ë‚´ë¶€ êµ¬ì¡°ì— ëŒ€í•´ì„œ  ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ Classificationì˜ ë‚´ë¶€ êµ¬ì¡°ì™€ ê°œë³„ layerë¥¼ í†µí•´ ë‚˜ì˜¤ëŠ” Output Tensorì˜ í¬ê¸°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. layerì˜ ìµœì¢… outputì˜ í¬ê¸°ëŠ” \[1,N\]ì´ë©°, ì—¬ê¸°ì„œ Nì€ num_labelsê³¼ ë™ì¼í•œ ê°’ì´ì ì‚°ì¶œí•´ì•¼í•˜ëŠ” ì¹´í…Œê³ ë¦¬ ê°œìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ë§Œì•½ Regression ìœ í˜•ì˜ outputì´ í•„ìš”í•œ ê²½ìš° N = 1ë¡œ ì„¤ì •í•´ì•¼ í•˜ë©°, kê°œì˜ ì¹´í…Œê³ ë¦¬ë¥¼ êµ¬ë¶„í•´ì•¼í•˜ëŠ” Outputì´ í•„ìš”í•œ ê²½ìš° N = kë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.

<p align = "center"><img src="https://yangoos57.github.io/static/0ed34c4ed6b114c93110fb7822142201/3c492/img8.png" width="600" height="500"/></p>
 
``` python
from torch import Tensor, nn
class classificationHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        classifier_dropout = (
            config.classifier_dropout
            if config.classifier_dropout is not None
            else config.hidden_dropout_prob
        )
        self.gelu = nn.functional.gelu
        self.dropout = nn.Dropout(classifier_dropout)
        # [batch, embed_size] => [batch, num_labels]
        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)
    def forward(self, features, **kwargs):
        x = features[:, 0, :] # [CLS] í† í° ì¶”ì¶œ
        x = self.dropout(x)
        x = self.dense(x)
        x = self.gelu(x)
        x = self.dropout(x)
        # label ê°œìˆ˜ë§Œí¼ ì°¨ì› ì¶•ì†Œ [batch, embed_size] => [batch, num_labels]
        x = self.out_proj(x)
        return x
```

### â– Cross-Encoder í•™ìŠµ

Cross-Encoderë¥¼ ì‹¤ì œ í•™ìŠµí•˜ëŠ” ê³¼ì •ì€ [Cross-Encoder í•™ìŠµ íŠœí† ë¦¬ì–¼(Jupyter Notebook)](https://github.com/yangoos57/Sentence_bert_from_scratch)ì„ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. í•´ë‹¹ íŠœí† ë¦¬ì–¼ì€ ğŸ¤— Transformersë¥¼ í™œìš©í•´ ì‘ì„±ë˜ì—ˆìœ¼ë¯€ë¡œ Huggingfaceì— ìµìˆ™í•˜ì§€ ì•Šìœ¼ì‹  ë¶„ë“¤ì€ ì¶”ê°€ì ìœ¼ë¡œ [ë§í¬](https://yangoos57.github.io/blog/DeepLearning/paper/Electra/electra/)ë¥¼ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.



## Bi-Encoder

ì´ì œ Sentence Bert ë…¼ë¬¸ì˜ í•µì‹¬ êµ¬ì¡°ì¸ Bi-Encoderì— ëŒ€í•´ ì„¤ëª…í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. Bi-EncoderëŠ” ë¬¸ì¥ ê°„ ë¹„êµê°€ í•„ìš”í•œ Taskì— ëŒ€í•´ í›¨ì‹  ë†’ì€ í¼í¬ë¨¼ìŠ¤ë¥¼ ë³´ì—¬ì£¼ëŠ” ì¥ì ì´ ìˆë‹¤ê³  ì„¤ëª…í•œ ë°” ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì†ë„ë¥¼ ë³´ì¥í•  ìˆ˜ ìˆëŠ” ì´ìœ ëŠ” Sentence Embeddingì„ í™œìš©í•´ ë¬¸ì¥ì„ ë²¡í„° ê³µê°„ì— ìœ„ì¹˜ì‹œì¼œ CosineSimilarityë¥¼ í™œìš©í•´ ê³„ì‚°í•˜ê¸° ë•Œë¬¸ì´ì—ˆìŠµë‹ˆë‹¤.

ì•„ë˜ í‘œ ì£¼í™©ìƒ‰ìœ¼ë¡œ ì³ì ¸ìˆëŠ” ì‹¤ì„  ì¤‘ Avg. Bert EmbeddingsëŠ” ì´ì „ì— ì‹œë„í–ˆë˜ Sentence Embedding ë°©ì‹ì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, ì´ëŸ¬í•œ ì„±ëŠ¥ì€ ê³¼ê±° ëª¨ë¸ì¸ Glove, InferSent ì„±ëŠ¥ì—ë„ ë¯¸ì¹˜ì§€ ëª»í•˜ê³  ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë°˜ë©´ NLI ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµí•œ SentenceBert ëª¨ë¸ì˜ ì„±ëŠ¥ì€ Glove, InferSent ì„±ëŠ¥ì„ ì••ë„í•  ë¿ë§Œì•„ë‹ˆë¼ ê¸°ì¡´ ë°©ì‹ì˜ ì„±ëŠ¥ ëŒ€ë¹„ ì•½ 1.8ë°° ì´ìƒì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<p align = "center"><img src="https://yangoos57.github.io/static/402c52b9e63859d06e0456b99dc4b571/13ae7/img2.png" width="500" height="600"/></p>



### â– Sentence Bert êµ¬ì¡° 

<p align = "center"><img src="https://yangoos57.github.io/static/39f1a72e77fc2a06fb0f0ccd8489a161/3d64b/img4.png" width="200" height="300"/></p>



 
``` python
from transformers import ElectraModel, ElectraTokenizer
import torch.nn as nn
import torch
model = ElectraModel.from_pretrained("monologg/koelectra-base-v3-discriminator")
tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-base-v3-discriminator")
class modelWithPooling(nn.Module):
    def __init__(self, model, pooling_type="mean") -> None:
        super().__init__()
        self.model = model  # base model ex)BertModel, ElectraModel ...
        self.pooling_type = pooling_type  # pooling type ì„¤ì •(ê¸°ë³¸ mean)
    def forward(self, **kwargs):
        features = self.model(**kwargs)
        # [batch_size, src_token, embed_size]
        attention_mask = kwargs["attention_mask"]
        last_hidden_state = features["last_hidden_state"]
        if self.pooling_type == "cls":
            """
            [cls] ë¶€ë¶„ë§Œ ì¶”ì¶œ
            """
            cls_token = last_hidden_state[:, 0]  # [batch_size, embed_size]
            result = cls_token
        if self.pooling_type == "max":
            """
            ë¬¸ì¥ ë‚´ í† í° ì¤‘ ê°€ì¥ ê°’ì´ í° tokenë§Œ ì¶”ì¶œ
            """
            input_mask_expanded = (
                attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
            )
            # Set padding tokens to large negative value
            last_hidden_state[input_mask_expanded == 0] = -1e9
            max_over_time = torch.max(last_hidden_state, 1)[0]
            result = max_over_time
        if self.pooling_type == "mean":
            """
            ë¬¸ì¥ ë‚´ í† í°ì„ í•©í•œ ë’¤ í‰ê· 
            """
            # padding ë¶€ë¶„ ì°¾ê¸° = [batch_size, src_token, embed_size]
            input_mask_expanded = (
                attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
            )
            # paddingì¸ ê²½ìš° 0 ì•„ë‹Œ ê²½ìš° 1ê³±í•œ ë’¤ ì´í•© = [batch_size, embed_size]
            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)
            # í‰ê·  ë‚´ê¸°ìœ„í•œ token ê°œìˆ˜
            sum_mask = input_mask_expanded.sum(1)
            sum_mask = torch.clamp(sum_mask, min=1e-9)
            result = sum_embeddings / sum_mask
        #  input.shape : [batch_size, src_token, embed_size] => output.shape : [batch_size, embed_size]
        return {"sentence_embedding": result}


```

### â– Sbert í•™ìŠµ êµ¬ì¡° : Categorical Dataë¥¼ í•™ìŠµí•˜ëŠ” ê²½ìš° 

SbertëŠ” í•™ìŠµì— í™œìš©ë  ë°ì´í„°ì…‹ì— ë”°ë¼ í•™ìŠµ êµ¬ì¡°ê°€ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ë”°ë¼ì„œ ìì‹ ì´ í™œìš©í•  ë°ì´í„°ì…‹ì´ numerical ë°ì´í„°ì…‹ì¸ì§€, categorical ë°ì´í„°ì…‹ì¸ì§€ êµ¬ë¶„ì„ í•´ì•¼í•©ë‹ˆë‹¤. ë¨¼ì € categorical ë°ì´í„° ìœ í˜•ì— ëŒ€í•´ì„œ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤. ì˜ˆì œì—ì„œ í™œìš©í•˜ëŠ” ë°ì´í„°ì…‹ì€ ìì—°ì–´ì¶”ë¡ (NLI) ë°ì´í„°ì…‹ì´ë©° êµ¬ì¡°ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

> {\'sen1\': \'ê·¸ë¦¬ê³  ê·¸ê°€ ë§í–ˆë‹¤, \"ì—„ë§ˆ, ì € ì™”ì–´ìš”.\"\',
> \'sen2\': \'ê·¸ëŠ” í•™êµ ë²„ìŠ¤ê°€ ê·¸ë¥¼ ë‚´ë ¤ì£¼ìë§ˆì ì—„ë§ˆì—ê²Œ ì „í™”ë¥¼
> ê±¸ì—ˆë‹¤.\',
> \'gold_label\': \'neutral\'}

categorical ë°ì´í„°ë¡œ Sbertë¥¼ í•™ìŠµí•˜ëŠ” êµ¬ì¡°ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. 1ì°¨ë¡œ SBert ëª¨ë¸ì„ í†µí•´ ì‚°ì¶œí•œ embedding vectorë¥¼ ê°ê° U,Vë¼ í•  ë•Œ U,V,\|U-V\|ë¥¼ í•˜ë‚˜ì˜ Tensorë¡œ concatì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ê·¸ ë‹¤ìŒ softmax Classifierë¥¼ í†µí•´ entailment, neutral, contraditionì„ íŒë‹¨í•˜ê³  Lossë¥¼ êµ¬í•´ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.


<p align = "center"><img src="https://yangoos57.github.io/static/4ce257bd3b28eebd860c628554145582/e17e5/img5.png" width="300" height="400"/></p>




#### â– categorical Data í•™ìŠµ êµ¬ì¡° 


 
``` python
from torch import nn
class modelForClassificationTraining(nn.Module):
    def __init__(self, model, *inputs, **kwargs):
        super().__init__()
        # í•™ìŠµí•  ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
        self.model = modelWithPooling(model)
        # ëª¨ë¸ embed_size
        sentence_embedding_dimension = self.model.model.config.hidden_size
        # concat í•´ì•¼í•˜ëŠ” vector ê°œìˆ˜(U,V, |U-V|)
        num_vectors_concatenated = 3
        # embed_size * 3 => 3 ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œì‹œí‚¤ëŠ” classifier
        self.classifier = nn.Linear(num_vectors_concatenated * sentence_embedding_dimension, 3)
    def forward(self, features, answer):
        """
        ìƒ´ ë„¤íŠ¸ì›Œí¬ëŠ” í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ë‘ ê°œì˜ outputì„ ì‚°ì¶œí•˜ëŠ” êµ¬ì¡°ì„.
        í•˜ë‚˜ì˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì§€ë§Œ ê°ê° ì¶œë ¥í•˜ë¯€ë¡œ Input ë°ì´í„° ìƒí˜¸ ê°„ ì˜í–¥ì„ ì¤„ ìˆ˜ ì—†ê²Œ ë¨.
        """
        # ê°œë³„ ë°ì´í„° ìƒì„±
        embeddings = [self.model(**input_data)["sentence_embedding"] for input_data in features]
        rep_a, rep_b = embeddings
        # U,V, |U-V| vector ë³‘í•©
        vectors_concat = []
        vectors_concat.append(rep_a)
        vectors_concat.append(rep_b)
        vectors_concat.append(torch.abs(rep_a - rep_b))
        features = torch.cat(vectors_concat, 1)
        # ë³‘í•©í•œ vector ì°¨ì› ì¶•ì†Œ
        outputs = self.classifier(features)
        # Loss ê³„ì‚°
        loss_fct = nn.CrossEntropyLoss()
        loss = loss_fct(outputs, answer.view(-1))
        return {"loss": loss}
```



### â– Sbert êµ¬ì¡° : Numerical Dataë¥¼ í•™ìŠµí•˜ëŠ” ê²½ìš° 

Numerical DataëŠ” ë¬¸ì¥ê³¼ ë¬¸ì¥ ê°„ ë¹„êµë¥¼ ìˆ˜ì¹˜ë£Œ í‘œí˜„í•œ ë°ì´í„°ë¥¼ ë§í•©ë‹ˆë‹¤.

> { \'sen1\': \'ë¹„í–‰ê¸°ê°€ ì´ë¥™í•˜ê³  ìˆë‹¤.\',
    \'sen2\': \'ë¹„í–‰ê¸°ê°€ ì´ë¥™í•˜ê³  ìˆë‹¤.\',
    \'score\': \'5.000\'}

Numerical í•™ìŠµ êµ¬ì¡°ëŠ” ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ í™œìš©í•´ Embedding Vectorë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
<p align = "center"><img src="https://yangoos57.github.io/static/9c9a98db74d4821476ca98bf435744f4/e17e5/img6.png" width="300" height="400"/></p>



#### â– Numerical Data í•™ìŠµ êµ¬ì¡°


 
``` python
from torch import nn
class modelForRegressionTraining(nn.Module):
    def __init__(self, model, *inputs, **kwargs):
        super().__init__()
        # í•™ìŠµì„ ìˆ˜í–‰í•  ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
        self.model = modelWithPooling(model)
    def forward(self, features, answer):
        # Sentence 1, Sentence 2ì— ëŒ€í•œ Embedding
        embeddings = [self.model(**input_data)["sentence_embedding"] for input_data in features]
        # Sentence 1, Sentence 2ì— ëŒ€í•œ Cosine Similarity ê³„ì‚°
        cos_score_transformation = nn.Identity()
        outputs = cos_score_transformation(torch.cosine_similarity(embeddings[0], embeddings[1]))
        # label score Normalization
        answer = answer / 5  # 0 ~ 5 => 0 ~ 1
        loss_fct = nn.MSELoss()
        loss = loss_fct(outputs, answer.view(-1))
        return {"loss": loss}

```


### Bi-Encoder í™œìš©

í•™ìŠµì´ ì™„ë£Œë˜ë©´ í•™ìŠµì— í™œìš©ëœ êµ¬ì¡°ëŠ” ë²„ë¦¬ê³  Sentence Bertë§Œ ì¶”ì¶œí•˜ì—¬ í™œìš©í•©ë‹ˆë‹¤. ì´ì™€ ê´€ë ¨í•œ ì˜ˆì œëŠ” [Sbert ê¹ƒí—ˆë¸Œ í˜ì´ì§€](https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications)ì— ì½”ë“œë¡œ ìì„¸íˆ ì„¤ëª…í•˜ê³  ìˆìœ¼ë‹ˆ ì‘ìš© ë°©ë²•ì— ëŒ€í•´ ê¶ê¸ˆí•œ ê²½ìš° í•´ë‹¹ ë§í¬ë¥¼ ì°¸ê³  ë°”ëë‹ˆë‹¤.


---

- Reference
    - https://yangoos57.github.io/blog/DeepLearning/paper/Sbert/Sbert/
    - https://hwiyong.tistory.com/392